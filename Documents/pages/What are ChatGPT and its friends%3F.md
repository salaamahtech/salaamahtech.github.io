- #Books, #ChatGPT, #[[Tech Concepts]]
-
-
- ![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781098152604/files/assets/cover.png){:height 414, :width 245}
-
- ChatGPT is a chat application that is built on LLMs like GPT-3, GPT-4, etc.
-
- What is a Language Model?
- Few known models?
	- Developed by OpenAI. GPT-2 is open-source. GPT-3 & GPT-4 are not open-source.
	- Microsoft's Sydney is model based on GPT-4 used in Bing search engine.
	- Microsoft's Kosmos-1 is trained on image content.
	- Google's LaMDA. Google's Bard chat UI is based on this.
	- Google's PaLM has 3x more parameters than LaMDA. Multimodal model used for robots and image content.
	- Google's Chinchilla
	- Anthropic's Claude. Poe is the chat UI
	- Meta's LLaMA
	- BigScience's BLOOM - open-source multilingual model
	- Stability AI's Stable Diffusion - generates images from text.
	- Github's Copilot - model to generate code from text is based on OpenAI Codex which is based on GPT-3.
		- GPT-3 provides a base "understanding" of English and several other human languages.
		- Codex is trained on the contents of StackOverflow and Github.
- What is a LLM (Large Language Model)?
	- A large language model is a trained deep-learning model that understands and generates text in a human-like fashion.
	- Behind the scene, it is a large transformer model that does all the magic.
	- https://machinelearningmastery.com/what-are-large-language-models/
- What is a Foundational Model?
	- The GPT-series LLMs are also called "foundation models."
	- Foundation models are a class of very powerful AI models that can be used as the basis for other models: they can be specialized, or retrained, or otherwise modified for specific applications.
	- While most of the foundation models people are talking about are LLMs, foundation models aren't limited to language: a generative art model like Stable Diffusion incorporates the ability to process language, but the ability to generate images belongs to an entirely different branch of AI.
- What are parameters?
	- Parameters are the internal variables that control the model's behavior. They are all “learned” during training, rather than set by the developers. It's commonly believed that the more parameters, the better.
	- GPT-3 has 175 billion parameters.
	- GPT-4 is believed to weigh in at least 3 or 4 times larger.
	- Google's LaMDA has 137 billion parameters.
	- PaLM has 540 billion parameters.
- What are Tokens?
	- For ChatGPT, the total length of the prompt and the response currently must be under 4096 tokens, where a token is a significant fraction of a word; a very long prompt forces ChatGPT to generate a shorter response.
	- ChatGPT's sense of “context”—the amount of text that it considers when it's in conversation—is measured in “tokens,” which are also used for billing. Tokens are significant parts of a word. OpenAI suggests two heuristics to convert word count to tokens: a token is 3/4 of a word, and a token is 4 letters. You can experiment with tokens using their Tokenizer tool. Some quick experiments show that root words in a compound word almost always count as tokens; suffixes (like “ility”) almost always count as tokens; the period at the end of a sentence (and other punctuation) often counts as a token; and an initial capital letter counts as a token (possibly to indicate the start of a sentence).
	- https://platform.openai.com/tokenizer
- What are prompts?
- Prompt Injection
	- Prompt injection is similar to SQL injection, in which an attacker inserts a malicious SQL statement into an application's entry field. Many applications built on language models use a hidden layer of prompts to tell the model what is and isn't allowed. In prompt injection, the attacker writes a prompt that tells the model to ignore any of its previous instructions, including this hidden layer. Prompt injection is used to get models to produce hate speech; it was used against Bing/Sydney to get Sydney to reveal its name, and to override instructions not to respond with copyrighted content or language that could be hurtful.
- What are responses?
- What are Transformers?
	- Transformers is a technology from Google Research and Google Brain from 2017.
	- Transformers are the building blocks for the AI models like GPT-4.
	- Transformers are a machine learning model architecture, like Long Short Term Memory Neutal Networks (LSTMs), and Convolutional Neural Networks (CNNs).
- What is RLHF?
	- Reinforcement Learning from Human Feedback (RLHF).
	- In RLHF, the model is given a number of prompts, and the results are evaluated by humans. This evaluation is converted into a score, which is then fed back into the training process. (In practice, humans are usually asked to compare the output from the model with no additional training to the current state of the trained model.)
	- https://huggingface.co/blog/rlhf
- What are A100 and H100 chips from NVIDIA?
- What are FLOPs?
	- Floating point operations per second
- Vector databases
- Diff b/w CPUs and GPUs?
- AI Hallucinations